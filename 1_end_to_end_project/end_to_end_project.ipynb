{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of an end to end ML project\n",
    "\n",
    "Source: Hands-On Machine Learning witch Scikit-Learn, Keras & Tensorflow by Aurelien Geron\n",
    "\n",
    "**Main steps**\n",
    "1. Look at the big picture\n",
    "2. Get the data\n",
    "3. Discover and visualize the data to gain insights\n",
    "4. Prepare data for ML algorithms\n",
    "5. Select a model and train it\n",
    "6. Fine-tune the model\n",
    "7. Present solution\n",
    "8. Lunch, monitor, and maintain your system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint Go thorugh a default check list. This works well for a lot of projects and gives you a starting piont that can be adpted to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Look at the big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Frame the problem.\n",
    "*Theory* <br>\n",
    "A. What is the business objective? How does your organization aims to benefit from the mode? A good understanding here will help to frame the problem, algorithms to use, performance measurements, and prioritization. Ask what the current solution is, if there is any. This will give you a reference for performance and what to solve. <br> \n",
    "B. What system to use? Supervised, unsupervised, reinforcement learning, classification, etc. <br>\n",
    "\n",
    "*Example* <br>\n",
    "A. The model should predict if it is worth it to invest in a given area or not (price estimates / market price). Current solution is done manually, time cosuming and costly. The error is ca. 20%. <br>\n",
    "B. Supervised learning task (given labled data-set), muliple univariant regression task (use of muliple features to predict a numeric value for each district), and plain batch learning (No data stream or need to rapidly adjust data and small enough to fit into memory).  \n",
    "\n",
    "#### 1.2 Select performance measure\n",
    "*Theory & Example* <br>\n",
    "Typical performance measure for a regression system is the Root Mean Square Error. Good estimate for errors with high weight on large errors. \n",
    "\n",
    "#### 1.3 Check Assumptions\n",
    "*Theory* <br>\n",
    "Check assumptions to avoide mistakes early on. (Communication!)\n",
    "\n",
    "*Example* <br>\n",
    "We predict prices but we need to check if the downstream team will not actually convert our estimates into categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data set - Set variables\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "# Download data set, unpack, and store data to local drive.\n",
    "from functions.get_data import fetch_housing_data\n",
    "fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into memory\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Quick look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at data\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics of data\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more quick infos about data shape and types\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations:* <br>\n",
    "Each row represents a district. There are 10 attributes. There are 20,640 instances in the dataset. Some columns (total_bedrooms) do not add up to that number => missing features. All attributes, excpet ocean_proximity, are numeric. We loaded a .csv to we know it must be text attribute if not numeric. Looking ar the first rows of ocean_proximity we see repition, is it a catergorical attribute? *Next step: Look at ocean_proximity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more ocean_proximity\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observation* <br>\n",
    "5 unique values over all instances => categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations* <br>\n",
    "1. Median income does not seem to be in USA. Check this => Data set is scaled and caped at 15 for higher income and 0.5 for lower income. These roughly represent 10.000USD this 3 == 30.000USD. *Note* this is a pre-processed attribute. <br>\n",
    "2. Housing median age and median income are also capped. This might our alogithm to think learn that prices never go beyong that limit. Check if it is necessary to predict behind the limit. IF Ture: either remove those instances from the dataset or gather more date (to inlcude data above the limit). <br>\n",
    "3. Attributes have different scales.\n",
    "4. Many histograms are tail-heavy. This makes it harder for some algorithms to detect (the correc) patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Create a test set\n",
    "\n",
    "Avoid **data snooping** as our brain is amazing at pattern detection thus might implement a bias early on. Thus we create a test-set early on of roughly 20% of the data set. It is important to always sample / split-off the same data so the model does not get exposed to all data though different runs. This has to be the case even on changes to the dataset. Next, random sampling might introduce a sampling bias. We need to **stratify** the sampling data. Make your you represent each strata accordingly (to few strats=no representation, to many=not enough data per strata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Median income is important featute => Straify data along.\n",
    "# Balance number of strate vs data/strata\n",
    "housing[\"income_cat\"] = pd.cut(\n",
    "    housing[\"median_income\"],\n",
    "    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "    labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off stratified (income_cat) test-set\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the split test set to the original test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observation* <br>\n",
    "The test set has the same proportions as the main set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the category again  # Artificially added data.\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Discover and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the training set to work with.\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the California image\n",
    "images_path = os.path.join(\"./\", \"images\")\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "filename = \"california.png\"\n",
    "print(\"Downloading\", filename)\n",
    "url = DOWNLOAD_ROOT + \"images/end_to_end_project/\" + filename\n",
    "urllib.request.urlretrieve(url, os.path.join(images_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "california_img=mpimg.imread(os.path.join(images_path, filename))\n",
    "ax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n",
    "                  s=housing['population']/100, label=\"Population\",\n",
    "                  c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n",
    "                  colorbar=False, alpha=0.4)\n",
    "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n",
    "           cmap=plt.get_cmap(\"jet\"))\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "\n",
    "prices = housing[\"median_house_value\"]\n",
    "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
    "cbar = plt.colorbar(ticks=tick_values/prices.max())\n",
    "cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n",
    "cbar.set_label('Median House Value', fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Correlations\n",
    "\n",
    "Remember, the correlation coefficient only measures linear correlatons and might miss out non linear correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pearson-correlation\n",
    "corr_matrix = housing.corr()\n",
    "\n",
    "# Correlation example of attributes to median_house_value correlation.\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pearson correlations via Pandas\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investiagate a promising attribute, median-income.\n",
    "\n",
    "*Observations*\n",
    "- We can see the 500 000 cap in the median_house_value.\n",
    "- Seemingly more caps at 480 000, 350 000 and others.\n",
    "- Clear upwards trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)\n",
    "plt.axis([0, 16, 0, 550000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Attribute combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Obervation* We can see that the bedroom to room ration is strong correlated to the median-house-value then total_rooms or total_bedrooms. This is an example of an insight you can gain by just playing the data. This step it not ment to be exaustive but a chance to play through some ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert to clean data set & seperate predictors and lables as we want to apply\n",
    "# different transformations to them.\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dealing with missing features:** <br>\n",
    "| Get rid of entries with missing feature <br>\n",
    "| Get rid of attribute <br>\n",
    "| Set a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn simple imputer to fill missing values fields.\n",
    "# Works only with numerical data, thus apply on a copy without ocean-proximity.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Initi imputer and set strategy.\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Create copy of purely numerical data.\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "# Fit imputer to these data.\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "# Transform training set\n",
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imputer is now trained, us it to fill the training set and future data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dealing with cateforical data**\n",
    "- Algorithms prefer numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OrdinalEncoder encodes catergorical attributes as numericals.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Problem:* The ML algorithm might assume that close numerical values are similar.\n",
    "A solution is one-hot-encoding, a sparse matrix with entries as rows and catergory\n",
    "(numerical) as columns where a 1 marks the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* To many categories might lead to performance problmes, evene with sparse\n",
    "matrices. We could try to find a nother useful numerical feature. i.e. distance\n",
    "to the ocean in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Custom Transformers\n",
    "\n",
    "At one point we will need to write our own transformators. Keep them in keep thier API consisten with the sklearn-API. Simply add 3 methods, transform(), fit(), and fit_transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example that adds the adds the combined attributes from before.\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# column index\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Example of a custome transformer.\n",
    "    \n",
    "        1 hyperparameter (add_beedrooms_per_room).\n",
    "        This transformer allows easy add (or not) add_beedrooms_per_room\n",
    "        to find out if adding this attributes gives some insight.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        \n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[\n",
    "                X,\n",
    "                rooms_per_household,\n",
    "                population_per_household,\n",
    "                bedrooms_per_room\n",
    "            ]        \n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "        \n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Feature scaling\n",
    "\n",
    "Algorithms do not perform well when features are not scaled. We have 2 options, min-max scaling (normalization) and standarization.\n",
    "\n",
    "- normalization: Values are shifted and rescaled to values between 0 and 1. ((x - min) / max);\n",
    "- standardization: Subtract the mean (standardized values always have a mean of 0) && devide by the standard-deviation so that the resulting distribution has a unit variance. ((x - mean) / std). Unlike min-max this does not bound values to specific ranges wich can be a problem for some algirithms. However, it is much less affected by outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Transformation pipelines\n",
    "Organize transformation steps execute in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipleline calls fit_transdormer() sequencially on all transformers.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundle columns transformers (numerical & catergorical).\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Select a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Try a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out on a few instances of test-set\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance on whole set with RMSE. NOT ON THE TEST SET!\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(f'Average prediction error: {lin_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68k USD error range with a median price of 120k to 265k is not very impressive. The model is underfiting. We can do better. This can mean the features provide not enough information or the model is not good enough. We can improve the model by feeding better features, reduce constrains, and change the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Try another model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DecisionTreeRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model. NOT ON THE TEST SET!\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error of 0. This can not be true. It is likely that the model overfited the data. We can only be sure if we test it on the test-set. However we only want to include it when we a certain about our model to avoide exposing the test data. So we use some training set data for training and some for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learns K-Fold cross-validation feature splits the code into 10 distinct subsets called folds, then iterates over these folds, use the selected fold as test-set and the others as training set and collects the performance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corss validate the DescisionTreeRegressor using K-Fold-10.\n",
    "# Expects an utility functions (higher is better) rather than a loss-function.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is worse then our LinearRegression. *Note* that we can see how accurate the predictions are (std)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LinearRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross valdiate LinearRegression\n",
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomForestRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test & Validate\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best of the three. However, the score on the training set is lower (better) then on the validation set => overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1 Grid search**\n",
    "Grid search different hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try differenct hyperparameter combination for RandomForestRegressor using gird-serach.\n",
    "# What theses parameters mean will be explained in another chapter.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {\n",
    "        'n_estimators': [3, 10, 30],   # We know the answer already,\n",
    "        'max_features': [2, 4, 8, 16], # but save some time.\n",
    "    },\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {\n",
    "        'bootstrap': [False],\n",
    "        'n_estimators': [3, 10],\n",
    "        'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "# Init regressor\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(\n",
    "    forest_reg,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fi\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best estimator => Hyperparameter setting\n",
    "param_comb = grid_search.best_params_\n",
    "cvrs = cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    if params == param_comb: \n",
    "        print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a better score then using the default hyperparamter settings. \n",
    "\n",
    "*Remember, we can include data preparations steps as hyperparamters.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomize Search**\n",
    "When the hyperparamter space is large, randomized search can be more conventient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble methodes**\n",
    "Bundle the models that perform the best. The group will often perform better then the inidvidual ones. This is especially true if they make different kinds of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze best models and thier errors**\n",
    "Some models let you investigate which parameters are important. (Sensitivity?). You may want to drop some of the less important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Evaluate on test set\n",
    "- We tweaked our models and think they are good enough to give them a try.\n",
    "- Nothing special here just remember NOT TO FIT! the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "# Select the data\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "# Feed to pipeline\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "# Predict\n",
    "final_prediction = final_model.predict(X_test_prepared)\n",
    "\n",
    "# Measure performance\n",
    "final_mse = mean_squared_error(y_test, final_prediction)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* Resist the tempation to fine tune your hyper-parameter to much. This would lead for them to be overtuned for the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You got an estimate how accuarte the model is but you might also want to find out how *precise* it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision of model\n",
    "from scipy import stats\n",
    "\n",
    "confidence = 0.95\n",
    "squared_errors = (final_prediction - y_test) ** 2\n",
    "np.sqrt(stats.t.interval(\n",
    "    confidence,\n",
    "    len(squared_errors) - 1,\n",
    "    loc=squared_errors.mean(),\n",
    "    scale=stats.sem(squared_errors)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "- What did we learn?\n",
    "- Assumptions?\n",
    "- Limitations?\n",
    "- What worked and what not?\n",
    "- Document everything.\n",
    "- Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Lunch, Monitor, and Maintain your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a Web-Api for it. Here a services that handles request-processing and another that only takes in a POST with the input, estimates, and returns the result.\n",
    "- You need monitoring, the performance. As a rule of thum the performane deteriates over time if not adopted. i.e. changing cameras.\n",
    "- Regulary collect fresh data and lable it.\n",
    "- Automate fine-tuning, deploying and monitoring.\n",
    "- Automate re-traing: Compate old and new model performance and choose the better one.\n",
    "- Monitor data-input.\n",
    "- Keep backups of model and data-set.\n",
    "- Your could create subset of test-set and see how the model performs on different sub-sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossary in order in which they appear during this chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines: Sequence of data processing components. They ususally work asynchronously and are connected via the data store. Here each component pulls data from a store, process is and stores the reults in another store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation: <br>\n",
    "\n",
    "m: number of instances in a dataset. <br>\n",
    "x^i: feature vector. <br>\n",
    "X: matrix containing all feature vectors of all dataset instances. <br>\n",
    "h: sytems prediction function. <br>\n",
    "RMSE(X,h): cost function. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
