{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f395fc-4f0c-4e88-ade2-8450600608aa",
   "metadata": {},
   "source": [
    "Reducing the dimensionality increasers performance at the cost at some information loss. This trade-off can be optimized. Are two featers strongly correlated? Take the average, this will lead to a lower dimensionality at a minor information loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739acb8a-861d-4567-871a-5acc843b0594",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pq/jm2zx9d53gzcv3l0yq26_ndh0000gn/T/ipykernel_10793/2831706954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Scikit-Learn ≥0.20 is required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m\"0.20\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"dim_reduction\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed4e219-0c80-4b97-bc4d-06946565eda2",
   "metadata": {},
   "source": [
    "# 1. The course of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bab59a-10f3-4680-8f42-c57aa90054fb",
   "metadata": {},
   "source": [
    "Things behave differenlty in higher dimensionality space.\n",
    "\n",
    "A random point in a 1x1 square will have a chance of 0.4% of being close to a border, in a 10 000 dim space this chance will be 99.999999%.\n",
    "\n",
    "In a 2D unit square the average distance of 2 points will be 0.52, in 3D 0.66, in 10 000D roughly 408.25. Training sets in higher dimensions are at risk to be very sparse. New instances will likely to be further away from exising onces making preducions less reliable. \n",
    "*The higher the dimensionality, the higher the risk of over fitting it*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea9bbd-d516-45c8-a694-1b580393d3b6",
   "metadata": {},
   "source": [
    "Possible solutions would be to get more data to increase the density. However, we scale exponentially so the number of instances needed and the comupational cost are ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21a269-ac87-454b-9e5e-1a26a9a07687",
   "metadata": {},
   "source": [
    "# 2. Main approach of dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f2b07-16d6-4492-9e62-4c5905067f60",
   "metadata": {},
   "source": [
    "## 2.1 Projection\n",
    "In practice most features are not uniformly spread out accross all dimensions. Many features are almost constantm while others are highly corrleated. As a result all training instances lie within (or close to) a much lower-dimensional subspace. So we can project them into a lower dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac361774-96eb-4d1d-a23b-72d892e94e5a",
   "metadata": {},
   "source": [
    "This does not always work as some subspaces twist and turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9acc2-bae2-4874-b4d6-277d511bbb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78adee1c-ef4c-46f4-bbec-c7d293c4ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot)\n",
    "ax.view_init(10, -70)\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e87fa-3664-46e9-b478-55ce349f9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.hot)\n",
    "plt.axis(axes[:4])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(t, X[:, 1], c=t, cmap=plt.cm.hot)\n",
    "plt.axis([4, 15, axes[2], axes[3]])\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913cf26-6b9d-4745-a34a-e69fe6c9064e",
   "metadata": {},
   "source": [
    "## 2.2 Manifold Learning\n",
    "\n",
    "Example: A 2D manifold is a 2D shape that can be twisted into higher-dimensional space. (i.e. Swiss role).\n",
    "\n",
    "General: A d-dimensional manifold is part of an n-dimensional space (where d < n), that locally resembles a d-dimensional hyperplane. \n",
    "\n",
    "**Manifold learning:** Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lies. It realies on the *manifold hypothesis* which holds that most real-world high dimesnional datasets lie clsoe to a much lower-dimensionality manifold. This often can be empirically observerd. \n",
    "\n",
    "*Example:* The MNIST hand written images all have things in common. They are all continouse white lines that are usually centered. Only a fraction of randomly selcted (active) pixels images would resemble a digit. The degree of freedome you have when creating a digit is way lower then the total possible degrees of freedome. Theses constrains squeeze the dataset into a lower dimension. \n",
    "\n",
    "*Another assumption:** Task at hand will be easier to solve then dimesnioanlity is reduced. This does not alway hold true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2990a60-31d5-4dc7-b6fe-5edb3628c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "\n",
    "x2s = np.linspace(axes[2], axes[3], 10)\n",
    "x3s = np.linspace(axes[4], axes[5], 10)\n",
    "x2, x3 = np.meshgrid(x2s, x3s)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot(111, projection='3d')\n",
    "\n",
    "positive_class = X[:, 0] > 5\n",
    "X_pos = X[positive_class]\n",
    "X_neg = X[~positive_class]\n",
    "ax.view_init(10, -70)\n",
    "ax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], \"y^\")\n",
    "ax.plot_wireframe(5, x2, x3, alpha=0.5)\n",
    "ax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], \"gs\")\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "plt.plot(t[positive_class], X[positive_class, 1], \"gs\")\n",
    "plt.plot(t[~positive_class], X[~positive_class, 1], \"y^\")\n",
    "plt.axis([4, 15, axes[2], axes[3]])\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot(111, projection='3d')\n",
    "\n",
    "positive_class = 2 * (t[:] - 4) > X[:, 1]\n",
    "X_pos = X[positive_class]\n",
    "X_neg = X[~positive_class]\n",
    "ax.view_init(10, -70)\n",
    "ax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], \"y^\")\n",
    "ax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], \"gs\")\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "plt.plot(t[positive_class], X[positive_class, 1], \"gs\")\n",
    "plt.plot(t[~positive_class], X[~positive_class, 1], \"y^\")\n",
    "plt.plot([4, 15], [0, 22], \"b-\", linewidth=2)\n",
    "plt.axis([4, 15, axes[2], axes[3]])\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0528b0d9-bac8-48b4-8e6f-f54aa2e36f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
